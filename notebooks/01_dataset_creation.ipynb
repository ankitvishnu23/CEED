{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 18:39:05.731094: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-09 18:39:05.731127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-09 18:39:05.731149: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-09 18:39:05.737613: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-09 18:39:06.598260: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/media/cat/cole/miniforge-pypy3/envs/ceed/lib/python3.10/site-packages/ibllib/atlas/__init__.py:202: DeprecationWarning: ibllib.atlas is deprecated. Please install iblatlas using \"pip install iblatlas\" and use this module instead\n",
      "  warnings.warn('ibllib.atlas is deprecated. Please install iblatlas using \"pip install iblatlas\" and use '\n"
     ]
    }
   ],
   "source": [
    "from analysis.data_gen_utils import download_IBL, extract_IBL, make_dataset, combine_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(385, 630000)\n",
      "\u001b[1;33m2023-10-09 18:39:19.525 WARNING  [spikeglx.py:104] /home/cole/Downloads/ONE/alyx.internationalbrainlab.org/cache/ap/danlab/Subjects/DY_016/2020-09-12/001/raw_ephys_data/probe00/chunk_000000_to_000020/_spikeglx_ephysData_g0_t0.imec0.ap.stream.cbin : meta data and compressed chunks dont checkout\n",
      "File duration: expected 3668.9780333333333, actual 21.0\n",
      "Will attempt to fudge the meta-data information.\u001b[0m\n",
      "WARNING:ibllib:/home/cole/Downloads/ONE/alyx.internationalbrainlab.org/cache/ap/danlab/Subjects/DY_016/2020-09-12/001/raw_ephys_data/probe00/chunk_000000_to_000020/_spikeglx_ephysData_g0_t0.imec0.ap.stream.cbin : meta data and compressed chunks dont checkout\n",
      "File duration: expected 3668.9780333333333, actual 21.0\n",
      "Will attempt to fudge the meta-data information.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "negative dimensions are not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m save_folder_sess1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/media/cat/data/IBL_data_CEED/dab512bd-a02d-4c1f-8dbc-9155a163efc0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m save_folder_sess2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/media/cat/data/IBL_data_CEED/febb430e-2d50-4f83-87a0-b5ffbb9a4943\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m bin_file_sess1, meta_file_sess1 \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_IBL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpid_sess1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_window\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_folder_sess1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m bin_file_sess2, meta_file_sess2 \u001b[38;5;241m=\u001b[39m download_IBL(pid\u001b[38;5;241m=\u001b[39mpid_sess2, t_window \u001b[38;5;241m=\u001b[39m t_window, save_folder\u001b[38;5;241m=\u001b[39msave_folder_sess2)\n",
      "File \u001b[0;32m~/CEED/analysis/data_gen_utils.py:513\u001b[0m, in \u001b[0;36mdownload_IBL\u001b[0;34m(pid, save_folder, t_window)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m# scans the file at constant interval, with a demi batch starting offset\u001b[39;00m\n\u001b[1;32m    510\u001b[0m nbatches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\n\u001b[1;32m    511\u001b[0m     np\u001b[38;5;241m.\u001b[39mfloor((sr\u001b[38;5;241m.\u001b[39mrl \u001b[38;5;241m-\u001b[39m batch_size_secs) \u001b[38;5;241m/\u001b[39m batch_intervals_secs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    512\u001b[0m )\n\u001b[0;32m--> 513\u001b[0m wrots \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnsync\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnsync\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ibatch \u001b[38;5;129;01min\u001b[39;00m trange(nbatches, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdestripe batches\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    515\u001b[0m     ifirst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\n\u001b[1;32m    516\u001b[0m         (ibatch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m*\u001b[39m batch_intervals_secs \u001b[38;5;241m*\u001b[39m sr\u001b[38;5;241m.\u001b[39mfs\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;241m+\u001b[39m batch_intervals_secs\n\u001b[1;32m    518\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: negative dimensions are not allowed"
     ]
    }
   ],
   "source": [
    "# download IBL data to a location and destripe/process using the session's pid\n",
    "pid_sess1 = 'dab512bd-a02d-4c1f-8dbc-9155a163efc0'\n",
    "pid_sess2 = 'febb430e-2d50-4f83-87a0-b5ffbb9a4943'\n",
    "t_window = [0, 21]\n",
    "save_folder_sess1 = '/media/cat/data/IBL_data_CEED/dab512bd-a02d-4c1f-8dbc-9155a163efc0'\n",
    "save_folder_sess2 = '/media/cat/data/IBL_data_CEED/febb430e-2d50-4f83-87a0-b5ffbb9a4943'\n",
    "bin_file_sess1, meta_file_sess1 = download_IBL(pid=pid_sess1, t_window = t_window, save_folder=save_folder_sess1)\n",
    "bin_file_sess2, meta_file_sess2 = download_IBL(pid=pid_sess2, t_window = t_window, save_folder=save_folder_sess2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract the necessary spike index and other items to make a dataset\n",
    "spike_idx_sess1, geom_sess1, chan_idx_sess1, templates_sess1 = extract_IBL(bin_fp=bin_file_sess1, meta_fp=meta_file_sess1, pid=pid_sess1, t_window=t_window)\n",
    "spike_idx_sess2, geom_sess2, chan_idx_sess2, templates_sess2 = extract_IBL(bin_fp=bin_file_sess2, meta_fp=meta_file_sess2, pid=pid_sess2, t_window=t_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session 1 units to get data from and dataset save path\n",
    "selected_units_sess1 = [10, 20, 30]\n",
    "dataset_folder_sess1 = '/ds1'\n",
    "\n",
    "# make a dataset for training\n",
    "# will create a folder with the spike, probe channel number, and corresponding channel location datasets in the train, val, test splits\n",
    "# optionally also saves out spatial and temporal noise covariance matrices\n",
    "make_dataset(bin_path=bin_file_sess1, spike_index=spike_idx_sess1, geom=geom_sess1, save_path=dataset_folder_sess1, chan_index=chan_idx_sess1, \n",
    "             templates=templates_sess1, unit_ids=selected_units_sess1, train_num=200, val_num=0, test_num=200, save_covs=True,\n",
    "             num_chans_extract=21, plot=False, normalize=False, shift=False, do_split=True,\n",
    "             save_var_num=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_units_sess2 = [100, 110, 120]\n",
    "dataset_folder_sess2 = '/ds2'\n",
    "\n",
    "# make dataset 2 for training\n",
    "make_dataset(bin_path=bin_file_sess2, spike_index=spike_idx_sess2, geom=geom_sess2, save_path=dataset_folder_sess2, chan_index=chan_idx_sess2, \n",
    "             templates=templates_sess2, unit_ids=selected_units_sess2, train_num=200, val_num=0, test_num=200, save_covs=True,\n",
    "             num_chans_extract=21, plot=False, normalize=False, shift=False, do_split=True, \n",
    "             save_var_num=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds_path = '/combined'\n",
    "\n",
    "# combine the two training datasets into a larger one for more unit diversity\n",
    "combine_datasets(dataset_folder_sess1, dataset_folder_sess2, combined_ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_ds_path = '/inference'\n",
    "selected_units_inf = [200, 210, 220]\n",
    "\n",
    "# make a dataset for spike sorting inference\n",
    "# will save out only spikes, channel numbers, and channel locations for a spikes test set (only pass in test_num)\n",
    "# save_var_num flag will allow units with < test_num spikes in the recording to be saved out in the test set as well (change to min and max spikes)\n",
    "# for cell type datasets the normalize Flag can be set to True\n",
    "make_dataset(bin_path=bin_file_sess1, spike_index=spike_idx_sess1, geom=geom_sess1, save_path=inference_ds_path, chan_index=chan_idx_sess1, \n",
    "             templates=templates_sess1, unit_ids=selected_units_inf, test_num=200, save_covs=True,\n",
    "             num_chans_extract=21, do_data_split=False, normalize=False, save_var_num=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ceed]",
   "language": "python",
   "name": "conda-env-ceed-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
