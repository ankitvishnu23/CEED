{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create 400 neuron dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import colorcet as cc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from analysis.data_gen_utils import (\n",
    "    all_units_except,\n",
    "    combine_datasets,\n",
    "    download_IBL,\n",
    "    extract_IBL,\n",
    "    make_dataset,\n",
    ")\n",
    "from analysis.projections import learn_manifold_umap, pca, pca_train\n",
    "from ceed.models.ceed import CEED\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_sess1 = 'dab512bd-a02d-4c1f-8dbc-9155a163efc0'\n",
    "pid_sess2 = 'febb430e-2d50-4f83-87a0-b5ffbb9a4943'\n",
    "save_folder_sess1 = \"/media/cat/data/IBL_data_CEED/dab512bd-a02d-4c1f-8dbc-9155a163efc0\"\n",
    "save_folder_sess2 = \"/media/cat/data/IBL_data_CEED/febb430e-2d50-4f83-87a0-b5ffbb9a4943\"\n",
    "t_window = [0, 1200]  # in seconds\n",
    "overwrite = False\n",
    "rec1, meta_file_sess1 = download_IBL(\n",
    "    pid=pid_sess1, t_window=t_window, save_folder=save_folder_sess1, overwrite=overwrite\n",
    ")\n",
    "rec2, meta_file_sess2 = download_IBL(\n",
    "    pid=pid_sess2, t_window=t_window, save_folder=save_folder_sess2, overwrite=overwrite\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"extract the all data needed to make CEED dataset\n",
    "spike_idx_sess: spike_times, channels, neurons (if use_labels=True)\n",
    "geom_sess: channels x 2\n",
    "chan_idx_sess: waveform extraction channels for each channel\n",
    "templates_sess: templates across all channels for all neurons\n",
    "\"\"\"\n",
    "recompute = True\n",
    "\n",
    "if recompute:\n",
    "    spike_idx_sess1, geom_sess1, chan_idx_sess1, templates_sess1 = extract_IBL(\n",
    "        rec=rec1,\n",
    "        meta_fp=meta_file_sess1,\n",
    "        pid=pid_sess1,\n",
    "        t_window=t_window,\n",
    "        use_labels=True,\n",
    "    )\n",
    "    spike_idx_sess2, geom_sess2, chan_idx_sess2, templates_sess2 = extract_IBL(\n",
    "        rec=rec2,\n",
    "        meta_fp=meta_file_sess2,\n",
    "        pid=pid_sess2,\n",
    "        t_window=t_window,\n",
    "        use_labels=True,\n",
    "    )\n",
    "    np.save(\"spike_idx_sess1.npy\", spike_idx_sess1)\n",
    "    np.save(\"geom_sess1.npy\", geom_sess1)\n",
    "    np.save(\"chan_idx_sess1.npy\", chan_idx_sess1)\n",
    "    np.save(\"templates_sess1.npy\", templates_sess1)\n",
    "    np.save(\"spike_idx_sess2.npy\", spike_idx_sess2)\n",
    "    np.save(\"geom_sess2.npy\", geom_sess2)\n",
    "    np.save(\"chan_idx_sess2.npy\", chan_idx_sess2)\n",
    "    np.save(\"templates_sess2.npy\", templates_sess2)\n",
    "else:\n",
    "    spike_idx_sess1 = np.load(\"spike_idx_sess1.npy\")\n",
    "    geom_sess1 = np.load(\"geom_sess1.npy\")\n",
    "    chan_idx_sess1 = np.load(\"chan_idx_sess1.npy\")\n",
    "    templates_sess1 = np.load(\"templates_sess1.npy\")\n",
    "    spike_idx_sess2 = np.load(\"spike_idx_sess2.npy\")\n",
    "    geom_sess2 = np.load(\"geom_sess2.npy\")\n",
    "    chan_idx_sess2 = np.load(\"chan_idx_sess2.npy\")\n",
    "    templates_sess2 = np.load(\"templates_sess2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session DYO16 units to get data from and dataset save path\n",
    "dy016_unit_ids_path = os.path.join(\n",
    "    os.getcwd(), \"400neuron_unit_ids\", \"dy016_unit_ids.npy\"\n",
    ")\n",
    "selected_units_sess1 = np.load(dy016_unit_ids_path)\n",
    "dataset_folder_sess1 = save_folder_sess1 + \"/ds\"\n",
    "\n",
    "# make first dataset for training with DY016 units\n",
    "# will create a folder with the spike, probe channel number, and corresponding channel location datasets in the train, val, test splits\n",
    "# optionally also saves out spatial and temporal noise covariance matrices\n",
    "inference = False\n",
    "train_num = 200\n",
    "val_num = 0\n",
    "test_num = 200\n",
    "save_covs = False\n",
    "num_chans_extract = 21\n",
    "normalize = False  # True for cell-type dataset\n",
    "shift = False\n",
    "save_fewer = False\n",
    "(\n",
    "    train_set1,\n",
    "    val_set1,\n",
    "    test_set1,\n",
    "    train_geom_locs1,\n",
    "    val_geom_locs1,\n",
    "    test_geom_locs1,\n",
    "    train_max_chan1,\n",
    "    val_max_chan1,\n",
    "    test_max_chan1,\n",
    ") = make_dataset(\n",
    "    rec=rec1,\n",
    "    spike_index=spike_idx_sess1,\n",
    "    geom=geom_sess1,\n",
    "    save_path=dataset_folder_sess1,\n",
    "    chan_index=chan_idx_sess1,\n",
    "    templates=templates_sess1,\n",
    "    unit_ids=selected_units_sess1,\n",
    "    train_num=train_num,\n",
    "    val_num=val_num,\n",
    "    test_num=test_num,\n",
    "    save_covs=save_covs,\n",
    "    num_chans_extract=num_chans_extract,\n",
    "    normalize=normalize,\n",
    "    shift=shift,\n",
    "    inference=inference,\n",
    "    save_fewer=save_fewer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session DYO16 units to get data from and dataset save path\n",
    "dy009_unit_ids_path = os.path.join(\n",
    "    os.getcwd(), \"400neuron_unit_ids\", \"dy009_unit_ids.npy\"\n",
    ")\n",
    "selected_units_sess2 = np.load(dy009_unit_ids_path)\n",
    "dataset_folder_sess2 = save_folder_sess2 + \"/ds\"\n",
    "\n",
    "# make second dataset for training with DY009 units\n",
    "# will create a folder with the spike, probe channel number, and corresponding channel location datasets in the train, val, test splits\n",
    "# optionally also saves out spatial and temporal noise covariance matrices\n",
    "inference = False\n",
    "train_num = 200\n",
    "val_num = 0\n",
    "test_num = 200\n",
    "save_covs = False\n",
    "num_chans_extract = 21\n",
    "normalize = False  # True for cell-type dataset\n",
    "shift = False\n",
    "save_fewer = False\n",
    "(\n",
    "    train_set2,\n",
    "    val_set2,\n",
    "    test_set2,\n",
    "    train_geom_locs2,\n",
    "    val_geom_locs2,\n",
    "    test_geom_locs2,\n",
    "    train_max_chan2,\n",
    "    val_max_chan2,\n",
    "    test_max_chan2,\n",
    ") = make_dataset(\n",
    "    rec=rec2,\n",
    "    spike_index=spike_idx_sess2,\n",
    "    geom=geom_sess2,\n",
    "    save_path=dataset_folder_sess2,\n",
    "    chan_index=chan_idx_sess2,\n",
    "    templates=templates_sess2,\n",
    "    unit_ids=selected_units_sess2,\n",
    "    train_num=train_num,\n",
    "    val_num=val_num,\n",
    "    test_num=test_num,\n",
    "    save_covs=save_covs,\n",
    "    num_chans_extract=num_chans_extract,\n",
    "    normalize=normalize,\n",
    "    shift=shift,\n",
    "    inference=inference,\n",
    "    save_fewer=save_fewer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds_path = \"/media/cat/data/IBL_data_CEED/400neuron_200spike_ds\"\n",
    "\n",
    "# combine the two training datasets into a larger one for more unit diversity\n",
    "dataset_list = [dataset_folder_sess1, dataset_folder_sess2]\n",
    "combine_datasets(dataset_list, combined_ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = combined_ds_path\n",
    "exp_name = \"spikesorting_CEED_400n_paper_experiment\"\n",
    "log_dir = data_dir + \"/logs/\"\n",
    "ckpt_dir = data_dir + \"/saved_models/\"\n",
    "batch_size = 512\n",
    "num_extra_chans = 5  # 11 channels total\n",
    "save_metrics = True\n",
    "epochs = 400\n",
    "aug_p_dict = {\n",
    "    \"collide\": 0.4,\n",
    "    \"crop_shift\": 0.5,\n",
    "    \"amp_jitter\": 0.7,\n",
    "    \"temporal_jitter\": 0.6,\n",
    "    \"smart_noise\": (0.5, 1.0),\n",
    "}\n",
    "# subsample the 10 neuron dataset used in the paper from the 400 neurons\n",
    "# will output the results on training metrics\n",
    "test_units = [11, 13, 16, 69, 84, 89, 277, 267, 332, 343]\n",
    "\n",
    "print(test_units)\n",
    "# Train the 400 neuron, 200 spike, 11 channel model benchmarked in the supplement of the paper\n",
    "# (very similar results to the 1200 spike version)\n",
    "ceed_test = CEED(num_extra_chans=5)\n",
    "ceed_test.train(\n",
    "    data_dir=data_dir,\n",
    "    exp_name=exp_name,\n",
    "    log_dir=log_dir,\n",
    "    epochs=epochs,\n",
    "    ckpt_dir=ckpt_dir,\n",
    "    batch_size=batch_size,\n",
    "    save_metrics=save_metrics,\n",
    "    aug_p_dict=aug_p_dict,\n",
    "    units_list=test_units,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/media/cat/data/IBL_data_CEED/400neuron_200spike_ds'\n",
    "ceed_test = CEED(num_extra_chans=5)\n",
    "ceed_test.load('/media/cat/data/IBL_data_CEED/400neuron_200spike_ds/saved_models/spikesorting_CEED_400n_paper_experiment/')\n",
    "fc_transformed_inference_data, fc_inference_labels = ceed_test.load_and_transform(\n",
    "    data_dir=data_dir,units_list=test_units, file_split=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# results are slightly different than paper with this generated dataset because the channel recording preprocessing/destriping\n",
    "# process has moved from the IBL functions to analagous ones from SpikeInterface. In order to benchmark with the original \n",
    "# test dataset please download the dataset from this link: https://uchicago.box.com/v/CEED-data-storage. You can then \n",
    "# perform inference on the test dataset in the folder using the model checkpoint created in this notebook. \n",
    "\n",
    "covariance_type = \"full\"\n",
    "n_clusters = 10\n",
    "reps_train = fc_transformed_inference_data\n",
    "reps_test = fc_transformed_inference_data\n",
    "scores = []\n",
    "for i in range(100):\n",
    "    gmm = GaussianMixture(n_clusters, random_state=i, covariance_type=covariance_type).fit(\n",
    "        reps_test\n",
    "    )\n",
    "    gmm_cont_test_labels = gmm.predict(reps_test)\n",
    "    score = adjusted_rand_score(fc_inference_labels, gmm_cont_test_labels) * 100\n",
    "    scores.append(score)\n",
    "    print(f\"num_comps: {fc_transformed_inference_data.shape[1]}, rand_score: {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ceed]",
   "language": "python",
   "name": "conda-env-ceed-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
